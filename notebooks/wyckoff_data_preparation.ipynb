{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Financial Market Data for Wyckoff Pattern Recognition with Transformers\n",
    "\n",
    "This notebook demonstrates the process of preparing financial market data for fine-tuning a transformer model to recognize Wyckoff patterns. We'll cover:\n",
    "\n",
    "1. Loading historical price and volume data\n",
    "2. Preprocessing and normalizing the data\n",
    "3. Labeling key Wyckoff events (accumulation, distribution, spring, upthrust)\n",
    "4. Converting the data into a format suitable for transformer model training\n",
    "5. Visualizing the processed data\n",
    "\n",
    "## Introduction to Wyckoff Methodology\n",
    "\n",
    "Richard Wyckoff was a pioneer in technical analysis who developed a methodology for understanding market behavior based on the actions of large institutional investors. The Wyckoff method focuses on identifying specific market phases and events:\n",
    "\n",
    "- **Accumulation**: A period where institutional investors are buying from retail traders\n",
    "- **Distribution**: A period where institutional investors are selling to retail traders\n",
    "- **Spring**: A price movement below support that quickly reverses, trapping sellers\n",
    "- **Upthrust**: A price movement above resistance that quickly reverses, trapping buyers\n",
    "\n",
    "Recognizing these patterns can provide valuable insights for trading decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment and Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import warnings\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Historical Market Data\n",
    "\n",
    "We'll use the `yfinance` library to download historical price and volume data for selected stocks. For Wyckoff analysis, we need a sufficient history to identify patterns, so we'll download several years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_market_data(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Load historical market data for a list of tickers\n",
    "    \n",
    "    Parameters:\n",
    "    tickers (list): List of ticker symbols\n",
    "    start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "    end_date (str): End date in 'YYYY-MM-DD' format\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary of DataFrames with OHLCV data for each ticker\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            print(f\"Downloading data for {ticker}...\")\n",
    "            data = yf.download(ticker, start=start_date, end=end_date)\n",
    "            \n",
    "            # Check if data was successfully downloaded\n",
    "            if len(data) > 0:\n",
    "                # Rename columns to standard format\n",
    "                data.columns = [col.lower().capitalize() for col in data.columns]\n",
    "                data.rename(columns={'Adj close': 'Adj_close'}, inplace=True)\n",
    "                \n",
    "                # Add ticker column\n",
    "                data['Ticker'] = ticker\n",
    "                \n",
    "                # Store in dictionary\n",
    "                data_dict[ticker] = data\n",
    "                print(f\"Downloaded {len(data)} rows for {ticker}\")\n",
    "            else:\n",
    "                print(f\"No data available for {ticker}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data for {ticker}: {e}\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Define tickers and date range\n",
    "tickers = ['SPY', 'AAPL', 'MSFT', 'AMZN', 'GOOGL']\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# Load data\n",
    "market_data = load_market_data(tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the data for one of the tickers to understand its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of data for the first ticker\n",
    "if market_data and len(market_data) > 0:\n",
    "    first_ticker = list(market_data.keys())[0]\n",
    "    print(f\"\\nSample data for {first_ticker}:\")\n",
    "    display(market_data[first_ticker].head())\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nBasic statistics for {first_ticker}:\")\n",
    "    display(market_data[first_ticker].describe())\n",
    "    \n",
    "    # Plot the price and volume data\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Plot price\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    ax1.plot(market_data[first_ticker].index, market_data[first_ticker]['Close'], 'b-', linewidth=1.5)\n",
    "    ax1.set_title(f'{first_ticker} Price History', fontsize=16)\n",
    "    ax1.set_ylabel('Price ($)', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot volume\n",
    "    ax2 = plt.subplot(2, 1, 2, sharex=ax1)\n",
    "    ax2.bar(market_data[first_ticker].index, market_data[first_ticker]['Volume'], color='gray', alpha=0.7)\n",
    "    ax2.set_title(f'{first_ticker} Volume History', fontsize=16)\n",
    "    ax2.set_ylabel('Volume', fontsize=14)\n",
    "    ax2.set_xlabel('Date', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data was loaded. Please check your internet connection or ticker symbols.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing and Feature Engineering\n",
    "\n",
    "Now we'll preprocess the data and engineer features that are relevant for Wyckoff analysis. This includes:\n",
    "\n",
    "1. Calculating technical indicators\n",
    "2. Handling missing values\n",
    "3. Creating features that capture price-volume relationships\n",
    "4. Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess market data and engineer features for Wyckoff analysis\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with OHLCV data\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Processed DataFrame with additional features\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # 1. Basic price and volume features\n",
    "    data['Range'] = data['High'] - data['Low']\n",
    "    data['Body'] = abs(data['Close'] - data['Open'])\n",
    "    data['Upper_shadow'] = data['High'] - data[['Open', 'Close']].max(axis=1)\n",
    "    data['Lower_shadow'] = data[['Open', 'Close']].min(axis=1) - data['Low']\n",
    "    \n",
    "    # 2. Price changes and returns\n",
    "    data['Price_change'] = data['Close'].diff()\n",
    "    data['Pct_change'] = data['Close'].pct_change()\n",
    "    data['Log_return'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "    \n",
    "    # 3. Moving averages\n",
    "    for window in [5, 10, 20, 50, 200]:\n",
    "        data[f'SMA_{window}'] = data['Close'].rolling(window=window).mean()\n",
    "        data[f'Volume_SMA_{window}'] = data['Volume'].rolling(window=window).mean()\n",
    "    \n",
    "    # 4. Price relative to moving averages\n",
    "    for window in [20, 50, 200]:\n",
    "        data[f'Price_to_SMA_{window}'] = data['Close'] / data[f'SMA_{window}']\n",
    "    \n",
    "    # 5. Volume features\n",
    "    data['Volume_change'] = data['Volume'].pct_change()\n",
    "    data['Relative_volume'] = data['Volume'] / data['Volume_SMA_20']\n",
    "    \n",
    "    # 6. Price-volume relationship\n",
    "    # Up days vs down days volume\n",
    "    data['Up_day'] = (data['Close'] > data['Close'].shift(1)).astype(int)\n",
    "    data['Down_day'] = (data['Close'] < data['Close'].shift(1)).astype(int)\n",
    "    data['Up_volume'] = data['Volume'] * data['Up_day']\n",
    "    data['Down_volume'] = data['Volume'] * data['Down_day']\n",
    "    \n",
    "    # 7. Volatility measures\n",
    "    for window in [5, 10, 20]:\n",
    "        data[f'Volatility_{window}'] = data['Pct_change'].rolling(window=window).std()\n",
    "    \n",
    "    # 8. Support and resistance levels\n",
    "    for window in [10, 20, 50]:\n",
    "        data[f'Support_{window}'] = data['Low'].rolling(window=window).min()\n",
    "        data[f'Resistance_{window}'] = data['High'].rolling(window=window).max()\n",
    "    \n",
    "    # 9. Distance from support/resistance\n",
    "    data['Dist_from_support'] = (data['Close'] - data['Support_20']) / data['Support_20']\n",
    "    data['Dist_from_resistance'] = (data['Resistance_20'] - data['Close']) / data['Close']\n",
    "    \n",
    "    # 10. Volume-weighted price\n",
    "    data['VWAP_daily'] = (data['Close'] * data['Volume']) / data['Volume']\n",
    "    \n",
    "    # 11. Effort vs Result (Wyckoff principle)\n",
    "    data['Effort_result_ratio'] = data['Relative_volume'] / (abs(data['Pct_change']) + 1e-10)\n",
    "    \n",
    "    # 12. Price trend features\n",
    "    data['Uptrend'] = ((data['Close'] > data['SMA_20']) & \n",
    "                       (data['SMA_20'] > data['SMA_50'])).astype(int)\n",
    "    data['Downtrend'] = ((data['Close'] < data['SMA_20']) & \n",
    "                         (data['SMA_20'] < data['SMA_50'])).astype(int)\n",
    "    data['Sideways'] = ((~data['Uptrend'].astype(bool)) & \n",
    "                        (~data['Downtrend'].astype(bool))).astype(int)\n",
    "    \n",
    "    # 13. Handle missing values\n",
    "    # Forward fill for NaN values in calculated columns\n",
    "    data = data.fillna(method='ffill')\n",
    "    # For any remaining NaNs, fill with zeros\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Process data for each ticker\n",
    "processed_data = {}\n",
    "for ticker, df in market_data.items():\n",
    "    print(f\"Processing data for {ticker}...\")\n",
    "    processed_data[ticker] = preprocess_data(df)\n",
    "    print(f\"Processed {ticker} data: {processed_data[ticker].shape[0]} rows, {processed_data[ticker].shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the engineered features for one of the tickers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of processed data for the first ticker\n",
    "if processed_data and len(processed_data) > 0:\n",
    "    first_ticker = list(processed_data.keys())[0]\n",
    "    print(f\"\\nSample processed data for {first_ticker}:\")\n",
    "    \n",
    "    # Select a subset of columns to display\n",
    "    columns_to_display = ['Open', 'High', 'Low', 'Close', 'Volume', \n",
    "                          'Range', 'Pct_change', 'SMA_20', 'Relative_volume', \n",
    "                          'Effort_result_ratio', 'Uptrend', 'Downtrend', 'Sideways']\n",
    "    \n",
    "    display(processed_data[first_ticker][columns_to_display].head())\n",
    "    \n",
    "    # Plot some of the engineered features\n",
    "    plt.figure(figsize=(16, 15))\n",
    "    \n",
    "    # Plot 1: Price with moving averages\n",
    "    ax1 = plt.subplot(3, 1, 1)\n",
    "    ax1.plot(processed_data[first_ticker].index, processed_data[first_ticker]['Close'], 'b-', linewidth=1.5, label='Close')\n",
    "    ax1.plot(processed_data[first_ticker].index, processed_data[first_ticker]['SMA_20'], 'g-', linewidth=1, label='SMA 20')\n",
    "    ax1.plot(processed_data[first_ticker].index, processed_data[first_ticker]['SMA_50'], 'r-', linewidth=1, label='SMA 50')\n",
    "    ax1.set_title(f'{first_ticker} Price with Moving Averages', fontsize=16)\n",
    "    ax1.set_ylabel('Price ($)', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Volume and relative volume\n",
    "    ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n",
    "    ax2.bar(processed_data[first_ticker].index, processed_data[first_ticker]['Volume'], color='gray', alpha=0.7, label='Volume')\n",
    "    ax2.set_ylabel('Volume', fontsize=14)\n",
    "    ax2_twin = ax2.twinx()\n",
    "    ax2_twin.plot(processed_data[first_ticker].index, processed_data[first_ticker]['Relative_volume'], 'r-', linewidth=1, label='Relative Volume')\n",
    "    ax2_twin.set_ylabel('Relative Volume', color='r', fontsize=14)\n",
    "    ax2_twin.tick_params(axis='y', labelcolor='r')\n",
    "    ax2.set_title(f'{first_ticker} Volume Analysis', fontsize=16)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add a legend for both axes\n",
    "    lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    # Plot 3: Effort-Result Ratio\n",
    "    ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n",
    "    ax3.plot(processed_data[first_ticker].index, processed_data[first_ticker]['Effort_result_ratio'], 'k-', linewidth=1)\n",
    "    ax3.axhline(y=processed_data[first_ticker]['Effort_result_ratio'].mean(), color='r', linestyle='--', alpha=0.7, label='Mean')\n",
    "    ax3.set_title(f'{first_ticker} Effort-Result Ratio (Wyckoff Principle)', fontsize=16)\n",
    "    ax3.set_ylabel('Effort/Result', fontsize=14)\n",
    "    ax3.set_xlabel('Date', fontsize=14)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No processed data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Labeling Wyckoff Patterns\n",
    "\n",
    "Now we'll implement algorithms to detect and label key Wyckoff patterns in our data. This is a critical step as these labels will serve as the ground truth for our transformer model.\n",
    "\n",
    "We'll focus on identifying four key Wyckoff patterns:\n",
    "1. Accumulation\n",
    "2. Distribution\n",
    "3. Spring\n",
    "4. Upthrust\n",
    "\n",
    "Note: Wyckoff pattern identification often involves subjective judgment. Our algorithm provides a systematic approach, but in practice, these patterns might be validated by experienced traders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_wyckoff_patterns(df, window_size=20):\n",
    "    \"\"\"\n",
    "    Label Wyckoff patterns in the data\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Processed DataFrame with technical indicators\n",
    "    window_size (int): Window size for pattern detection\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with added pattern labels\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Initialize pattern columns\n",
    "    data['Accumulation'] = 0\n",
    "    data['Distribution'] = 0\n",
    "    data['Spring'] = 0\n",
    "    data['Upthrust'] = 0\n",
    "    data['No_Pattern'] = 1  # Default is no pattern\n",
    "    \n",
    "    # We need sufficient history for pattern detection\n",
    "    min_idx = max(window_size * 2, 50)\n",
    "    \n",
    "    # 1. Detect Accumulation\n",
    "    # Characteristics: Sideways price action after downtrend, increased volume on up days\n",
    "    for i in range(min_idx, len(data)):\n",
    "        # Check for prior downtrend\n",
    "        prior_trend = data['Downtrend'].iloc[i-window_size:i].mean() > 0.7\n",
    "        \n",
    "        # Check for current sideways movement\n",
    "        current_sideways = data['Sideways'].iloc[i-window_size//2:i].mean() > 0.7\n",
    "        \n",
    "        # Check volume characteristics\n",
    "        up_vol = data['Up_volume'].iloc[i-window_size//2:i].mean()\n",
    "        down_vol = data['Down_volume'].iloc[i-window_size//2:i].mean()\n",
    "        vol_characteristic = up_vol > down_vol * 1.2\n",
    "        \n",
    "        # Price range is contracting\n",
    "        range_contracting = (data['Range'].iloc[i-window_size//2:i].mean() < \n",
    "                            data['Range'].iloc[i-window_size:i-window_size//2].mean())\n",
    "        \n",
    "        # Label as accumulation if all conditions are met\n",
    "        if prior_trend and current_sideways and vol_characteristic and range_contracting:\n",
    "            # Label a sequence of days as accumulation\n",
    "            data['Accumulation'].iloc[i-window_size//2:i+1] = 1\n",
    "            data['No_Pattern'].iloc[i-window_size//2:i+1] = 0\n",
    "    \n",
    "    # 2. Detect Distribution\n",
    "    # Characteristics: Sideways price action after uptrend, increased volume on down days\n",
    "    for i in range(min_idx, len(data)):\n",
    "        # Check for prior uptrend\n",
    "        prior_trend = data['Uptrend'].iloc[i-window_size:i].mean() > 0.7\n",
    "        \n",
    "        # Check for current sideways movement\n",
    "        current_sideways = data['Sideways'].iloc[i-window_size//2:i].mean() > 0.7\n",
    "        \n",
    "        # Check volume characteristics\n",
    "        up_vol = data['Up_volume'].iloc[i-window_size//2:i].mean()\n",
    "        down_vol = data['Down_volume'].iloc[i-window_size//2:i].mean()\n",
    "        vol_characteristic = down_vol > up_vol * 1.2\n",
    "        \n",
    "        # Price range is contracting\n",
    "        range_contracting = (data['Range'].iloc[i-window_size//2:i].mean() < \n",
    "                            data['Range'].iloc[i-window_size:i-window_size//2].mean())\n",
    "        \n",
    "        # Label as distribution if all conditions are met\n",
    "        if prior_trend and current_sideways and vol_characteristic and range_contracting:\n",
    "            # Label a sequence of days as distribution\n",
    "            data['Distribution'].iloc[i-window_size//2:i+1] = 1\n",
    "            data['No_Pattern'].iloc[i-window_size//2:i+1] = 0\n",
    "    \n",
    "    # 3. Detect Spring\n",
    "    # Characteristics: Price drops below recent support and quickly recovers\n",
    "    for i in range(min_idx, len(data)):\n",
    "        # Check if price dropped below recent support\n",
    "        support_level = data['Support_20'].iloc[i-1]\n",
    "        price_dropped = data['Low'].iloc[i] < support_level * 0.99\n",
    "        \n",
    "        # Check for quick recovery\n",
    "        if price_dropped and i+5 < len(data):\n",
    "            recovery = data['Close'].iloc[i+5] > support_level\n",
    "            volume_surge = data['Volume'].iloc[i] > data['Volume_SMA_20'].iloc[i] * 1.5\n",
    "            \n",
    "            # Label as spring if conditions are met\n",
    "            if recovery and volume_surge:\n",
    "                # Label the spring day and a few days after\n",
    "                data['Spring'].iloc[i:i+3] = 1\n",
    "                data['No_Pattern'].iloc[i:i+3] = 0\n",
    "    \n",
    "    # 4. Detect Upthrust\n",
    "    # Characteristics: Price rises above recent resistance and quickly fails\n",
    "    for i in range(min_idx, len(data)):\n",
    "        # Check if price rose above recent resistance\n",
    "        resistance_level = data['Resistance_20'].iloc[i-1]\n",
    "        price_rose = data['High'].iloc[i] > resistance_level * 1.01\n",
    "        \n",
    "        # Check for quick failure\n",
    "        if price_rose and i+5 < len(data):\n",
    "            failure = data['Close'].iloc[i+5] < resistance_level\n",
    "            volume_surge = data['Volume'].iloc[i] > data['Volume_SMA_20'].iloc[i] * 1.5\n",
    "            \n",
    "            # Label as upthrust if conditions are met\n",
    "            if failure and volume_surge:\n",
    "                # Label the upthrust day and a few days after\n",
    "                data['Upthrust'].iloc[i:i+3] = 1\n",
    "                data['No_Pattern'].iloc[i:i+3] = 0\n",
    "    \n",
    "    # Ensure no overlapping patterns (prioritize in order: Spring, Upthrust, Accumulation, Distribution)\n",
    "    for i in range(len(data)):\n",
    "        if data['Spring'].iloc[i] == 1:\n",
    "            data['Upthrust'].iloc[i] = 0\n",
    "            data['Accumulation'].iloc[i] = 0\n",
    "            data['Distribution'].iloc[i] = 0\n",
    "        elif data['Upthrust'].iloc[i] == 1:\n",
    "            data['Accumulation'].iloc[i] = 0\n",
    "            data['Distribution'].iloc[i] = 0\n",
    "        elif data['Accumulation'].iloc[i] == 1:\n",
    "            data['Distribution'].iloc[i] = 0\n",
    "    \n",
    "    # Create a single pattern label column\n",
    "    # 0: No Pattern, 1: Accumulation, 2: Distribution, 3: Spring, 4: Upthrust\n",
    "    data['Pattern_Label'] = 0\n",
    "    data.loc[data['Accumulation'] == 1, 'Pattern_Label'] = 1\n",
    "    data.loc[data['Distribution'] == 1, 'Pattern_Label'] = 2\n",
    "    data.loc[data['Spring'] == 1, 'Pattern_Label'] = 3\n",
    "    data.loc[data['Upthrust'] == 1, 'Pattern_Label'] = 4\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Label patterns for each ticker\n",
    "labeled_data = {}\n",
    "for ticker, df in processed_data.items():\n",
    "    print(f\"Labeling Wyckoff patterns for {ticker}...\")\n",
    "    labeled_data[ticker] = label_wyckoff_patterns(df)\n",
    "    \n",
    "    # Count patterns\n",
    "    pattern_counts = {\n",
    "        'No Pattern': (labeled_data[ticker]['Pattern_Label'] == 0).sum(),\n",
    "        'Accumulation': (labeled_data[ticker]['Pattern_Label'] == 1).sum(),\n",
    "        'Distribution': (labeled_data[ticker]['Pattern_Label'] == 2).sum(),\n",
    "        'Spring': (labeled_data[ticker]['Pattern_Label'] == 3).sum(),\n",
    "        'Upthrust': (labeled_data[ticker]['Pattern_Label'] == 4).sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"Pattern counts for {ticker}: {pattern_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the labeled patterns for one of the tickers to verify our labeling algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_labeled_patterns(ticker, data):\n",
    "    \"\"\"\n",
    "    Visualize the labeled Wyckoff patterns\n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): Ticker symbol\n",
    "    data (DataFrame): DataFrame with labeled patterns\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Plot price\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    ax1.plot(data.index, data['Close'], 'b-', linewidth=1.5)\n",
    "    \n",
    "    # Add moving averages\n",
    "    ax1.plot(data.index, data['SMA_20'], 'g--', linewidth=1, alpha=0.7, label='SMA 20')\n",
    "    ax1.plot(data.index, data['SMA_50'], 'r--', linewidth=1, alpha=0.7, label='SMA 50')\n",
    "    \n",
    "    # Highlight patterns\n",
    "    # Accumulation (blue)\n",
    "    accumulation_idx = data.index[data['Accumulation'] == 1]\n",
    "    if len(accumulation_idx) > 0:\n",
    "        ax1.scatter(accumulation_idx, data.loc[accumulation_idx, 'Close'], \n",
    "                   color='blue', marker='o', s=50, label='Accumulation')\n",
    "    \n",
    "    # Distribution (red)\n",
    "    distribution_idx = data.index[data['Distribution'] == 1]\n",
    "    if len(distribution_idx) > 0:\n",
    "        ax1.scatter(distribution_idx, data.loc[distribution_idx, 'Close'], \n",
    "                   color='red', marker='o', s=50, label='Distribution')\n",
    "    \n",
    "    # Spring (green)\n",
    "    spring_idx = data.index[data['Spring'] == 1]\n",
    "    if len(spring_idx) > 0:\n",
    "        ax1.scatter(spring_idx, data.loc[spring_idx, 'Low'], \n",
    "                   color='green', marker='^', s=100, label='Spring')\n",
    "    \n",
    "    # Upthrust (purple)\n",
    "    upthrust_idx = data.index[data['Upthrust'] == 1]\n",
    "    if len(upthrust_idx) > 0:\n",
    "        ax1.scatter(upthrust_idx, data.loc[upthrust_idx, 'High'], \n",
    "                   color='purple', marker='v', s=100, label='Upthrust')\n",
    "    \n",
    "    ax1.set_title(f'{ticker} Price with Labeled Wyckoff Patterns', fontsize=16)\n",
    "    ax1.set_ylabel('Price ($)', fontsize=14)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot volume\n",
    "    ax2 = plt.subplot(2, 1, 2, sharex=ax1)\n",
    "    ax2.bar(data.index, data['Volume'], color='gray', alpha=0.7)\n",
    "    ax2.plot(data.index, data['Volume_SMA_20'], 'r--', linewidth=1, alpha=0.7, label='Volume SMA 20')\n",
    "    \n",
    "    # Highlight volume for pattern days\n",
    "    pattern_idx = data.index[data['Pattern_Label'] > 0]\n",
    "    if len(pattern_idx) > 0:\n",
    "        ax2.bar(pattern_idx, data.loc[pattern_idx, 'Volume'], color='red', alpha=0.7)\n",
    "    \n",
    "    ax2.set_title(f'{ticker} Volume', fontsize=16)\n",
    "    ax2.set_ylabel('Volume', fontsize=14)\n",
    "    ax2.set_xlabel('Date', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also show a zoomed-in view of a specific pattern if available\n",
    "    pattern_types = ['Accumulation', 'Distribution', 'Spring', 'Upthrust']\n",
    "    for pattern in pattern_types:\n",
    "        pattern_idx = data.index[data[pattern] == 1]\n",
    "        if len(pattern_idx) > 0:\n",
    "            # Get a continuous segment of the pattern\n",
    "            segments = []\n",
    "            current_segment = []\n",
    "            \n",
    "            for i in range(len(pattern_idx)):\n",
    "                if i == 0 or (pattern_idx[i] - pattern_idx[i-1]).days <= 5:  # Allow small gaps\n",
    "                    current_segment.append(pattern_idx[i])\n",
    "                else:\n",
    "                    if current_segment:\n",
    "                        segments.append(current_segment)\n",
    "                    current_segment = [pattern_idx[i]]\n",
    "            \n",
    "            if current_segment:\n",
    "                segments.append(current_segment)\n",
    "            \n",
    "            # Take the longest segment\n",
    "            if segments:\n",
    "                longest_segment = max(segments, key=len)\n",
    "                \n",
    "                # Define window for zoomed view (add context before and after)\n",
    "                start_date = longest_segment[0] - pd.Timedelta(days=20)\n",
    "                end_date = longest_segment[-1] + pd.Timedelta(days=20)\n",
    "                \n",
    "                # Filter data for the window\n",
    "                window_data = data.loc[start_date:end_date]\n",
    "                \n",
    "                # Plot zoomed view\n",
    "                plt.figure(figsize=(16, 8))\n",
    "                \n",
    "                # Plot price\n",
    "                ax = plt.subplot(2, 1, 1)\n",
    "                ax.plot(window_data.index, window_data['Close'], 'b-', linewidth=1.5)\n",
    "                \n",
    "                # Add moving averages\n",
    "                ax.plot(window_data.index, window_data['SMA_20'], 'g--', linewidth=1, alpha=0.7, label='SMA 20')\n",
    "                \n",
    "                # Highlight pattern days\n",
    "                pattern_days = window_data.index[window_data[pattern] == 1]\n",
    "                if pattern == 'Spring':\n",
    "                    ax.scatter(pattern_days, window_data.loc[pattern_days, 'Low'], \n",
    "                              color='green', marker='^', s=100, label=pattern)\n",
    "                elif pattern == 'Upthrust':\n",
    "                    ax.scatter(pattern_days, window_data.loc[pattern_days, 'High'], \n",
    "                              color='purple', marker='v', s=100, label=pattern)\n",
    "                else:\n",
    "                    ax.scatter(pattern_days, window_data.loc[pattern_days, 'Close'], \n",
    "                              color='blue' if pattern == 'Accumulation' else 'red', \n",
    "                              marker='o', s=50, label=pattern)\n",
    "                \n",
    "                # Shade the pattern region\n",
    "                min_price = window_data['Low'].min() * 0.98\n",
    "                max_price = window_data['High'].max() * 1.02\n",
    "                \n",
    "                for day in pattern_days:\n",
    "                    ax.axvspan(day - pd.Timedelta(hours=12), day + pd.Timedelta(hours=12), \n",
    "                              alpha=0.2, color='green' if pattern == 'Spring' else \n",
    "                                         'purple' if pattern == 'Upthrust' else\n",
    "                                         'blue' if pattern == 'Accumulation' else 'red')\n",
    "                \n",
    "                ax.set_title(f'{ticker} {pattern} Pattern (Zoomed View)', fontsize=16)\n",
    "                ax.set_ylabel('Price ($)', fontsize=14)\n",
    "                ax.set_ylim(min_price, max_price)\n",
    "                ax.legend(loc='upper left')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Plot volume\n",
    "                ax2 = plt.subplot(2, 1, 2, sharex=ax)\n",
    "                ax2.bar(window_data.index, window_data['Volume'], color='gray', alpha=0.7)\n",
    "                ax2.plot(window_data.index, window_data['Volume_SMA_20'], 'r--', linewidth=1, alpha=0.7, label='Volume SMA 20')\n",
    "                \n",
    "                # Highlight volume for pattern days\n",
    "                ax2.bar(pattern_days, window_data.loc[pattern_days, 'Volume'], \n",
    "                       color='green' if pattern == 'Spring' else \n",
    "                             'purple' if pattern == 'Upthrust' else\n",
    "                             'blue' if pattern == 'Accumulation' else 'red', alpha=0.7)\n",
    "                \n",
    "                ax2.set_title(f'Volume during {pattern}', fontsize=16)\n",
    "                ax2.set_ylabel('Volume', fontsize=14)\n",
    "                ax2.set_xlabel('Date', fontsize=14)\n",
    "                ax2.legend()\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Only show one example of each pattern type\n",
    "                break\n",
    "\n",
    "# Visualize patterns for the first ticker\n",
    "if labeled_data and len(labeled_data) > 0:\n",
    "    first_ticker = list(labeled_data.keys())[0]\n",
    "    visualize_labeled_patterns(first_ticker, labeled_data[first_ticker])\n",
    "else:\n",
    "    print(\"No labeled data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparing Data for Transformer Model Training\n",
    "\n",
    "Now we'll prepare the labeled data for training a transformer model. This involves:\n",
    "\n",
    "1. Creating sequences of fixed length\n",
    "2. Normalizing the features\n",
    "3. Splitting the data into training and validation sets\n",
    "4. Converting the data into PyTorch tensors\n",
    "\n",
    "Transformers typically work with sequences, so we'll create sliding windows of data to capture the temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, seq_length=60, step=1, label_col='Pattern_Label'):\n",
    "    \"\"\"\n",
    "    Create sequences of fixed length from the data\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with features and labels\n",
    "    seq_length (int): Length of each sequence\n",
    "    step (int): Step size for sliding window\n",
    "    label_col (str): Name of the label column\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X, y) where X is the feature sequences and y is the labels\n",
    "    \"\"\"\n",
    "    # Select features (exclude date index, ticker, and label columns)\n",
    "    feature_cols = [col for col in df.columns if col not in ['Ticker', 'Pattern_Label', \n",
    "                                                             'Accumulation', 'Distribution', \n",
    "                                                             'Spring', 'Upthrust', 'No_Pattern']]\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    data = df[feature_cols].values\n",
    "    labels = df[label_col].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(0, len(data) - seq_length, step):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        # Use the label at the end of the sequence\n",
    "        y.append(labels[i+seq_length-1])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def normalize_sequences(X_train, X_val):\n",
    "    \"\"\"\n",
    "    Normalize sequences using StandardScaler\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (ndarray): Training sequences\n",
    "    X_val (ndarray): Validation sequences\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train_norm, X_val_norm, scaler)\n",
    "    \"\"\"\n",
    "    # Reshape to 2D for scaling\n",
    "    X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "    \n",
    "    # Fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "    X_train_2d_scaled = scaler.fit_transform(X_train_2d)\n",
    "    \n",
    "    # Transform validation data\n",
    "    X_val_2d_scaled = scaler.transform(X_val_2d)\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_train_scaled = X_train_2d_scaled.reshape(X_train.shape)\n",
    "    X_val_scaled = X_val_2d_scaled.reshape(X_val.shape)\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, scaler\n",
    "\n",
    "def prepare_data_for_transformer(labeled_data, seq_length=60, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for transformer model training\n",
    "    \n",
    "    Parameters:\n",
    "    labeled_data (dict): Dictionary of DataFrames with labeled data\n",
    "    seq_length (int): Length of each sequence\n",
    "    val_size (float): Proportion of data to use for validation\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, y_train, X_val, y_val, scaler)\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    \n",
    "    # Combine data from all tickers\n",
    "    for ticker, df in labeled_data.items():\n",
    "        X, y = create_sequences(df, seq_length=seq_length)\n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    \n",
    "    # Concatenate data\n",
    "    X = np.concatenate(all_X, axis=0)\n",
    "    y = np.concatenate(all_y, axis=0)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=42, stratify=y)\n",
    "    \n",
    "    # Normalize the data\n",
    "    X_train_scaled, X_val_scaled, scaler = normalize_sequences(X_train, X_val)\n",
    "    \n",
    "    print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "    print(f\"Validation data shape: {X_val_scaled.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Validation labels shape: {y_val.shape}\")\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    \n",
    "    return X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, scaler\n",
    "\n",
    "# Prepare data for transformer model\n",
    "X_train, y_train, X_val, y_val, scaler = prepare_data_for_transformer(labeled_data, seq_length=60)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the distribution of labels in our training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_distribution(y_train, y_val):\n",
    "    \"\"\"\n",
    "    Plot the distribution of labels in training and validation sets\n",
    "    \n",
    "    Parameters:\n",
    "    y_train (Tensor): Training labels\n",
    "    y_val (Tensor): Validation labels\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    y_train_np = y_train.numpy()\n",
    "    y_val_np = y_val.numpy()\n",
    "    \n",
    "    # Count labels\n",
    "    train_counts = np.bincount(y_train_np)\n",
    "    val_counts = np.bincount(y_val_np)\n",
    "    \n",
    "    # Ensure both arrays have the same length\n",
    "    max_len = max(len(train_counts), len(val_counts))\n",
    "    if len(train_counts) < max_len:\n",
    "        train_counts = np.pad(train_counts, (0, max_len - len(train_counts)), 'constant')\n",
    "    if len(val_counts) < max_len:\n",
    "        val_counts = np.pad(val_counts, (0, max_len - len(val_counts)), 'constant')\n",
    "    \n",
    "    # Create labels\n",
    "    labels = ['No Pattern', 'Accumulation', 'Distribution', 'Spring', 'Upthrust']\n",
    "    if len(labels) < max_len:\n",
    "        labels.extend([f'Class {i}' for i in range(len(labels), max_len)])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, train_counts, width, label='Training')\n",
    "    plt.bar(x + width/2, val_counts, width, label='Validation')\n",
    "    \n",
    "    plt.xlabel('Pattern Class', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.title('Distribution of Pattern Classes in Training and Validation Sets', fontsize=16)\n",
    "    plt.xticks(x, labels)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add counts as text\n",
    "    for i, count in enumerate(train_counts):\n",
    "        plt.text(i - width/2, count + 5, str(count), ha='center')\n",
    "    \n",
    "    for i, count in enumerate(val_counts):\n",
    "        plt.text(i + width/2, count + 5, str(count), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot label distribution\n",
    "plot_label_distribution(y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating a Simple Transformer Model Architecture\n",
    "\n",
    "Now let's define a simple transformer model architecture that can be used for Wyckoff pattern recognition. This is a starting point that can be further refined and fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        # Register buffer (not a parameter, but should be part of the module's state)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input\n",
    "        # x shape: [seq_len, batch_size, d_model]\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class WyckoffTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, num_classes, dropout=0.1):\n",
    "        super(WyckoffTransformer, self).__init__()\n",
    "        \n",
    "        # Feature embedding\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(d_model, d_model // 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(d_model // 2, num_classes)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, seq_len, features]\n",
    "        \n",
    "        # Transpose for transformer: [seq_len, batch_size, features]\n",
    "        src = src.permute(1, 0, 2)\n",
    "        \n",
    "        # Embed features\n",
    "        src = self.embedding(src)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        output = self.transformer_encoder(src)\n",
    "        \n",
    "        # Use the output of the last timestep for classification\n",
    "        output = output[-1]\n",
    "        \n",
    "        # Pass through final layers\n",
    "        output = torch.relu(self.fc1(output))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = X_train.shape[2]  # Number of features\n",
    "d_model = 64  # Embedding dimension\n",
    "nhead = 4  # Number of attention heads\n",
    "num_layers = 2  # Number of transformer layers\n",
    "num_classes = len(torch.unique(y_train))  # Number of pattern classes\n",
    "dropout = 0.2\n",
    "\n",
    "# Initialize model\n",
    "model = WyckoffTransformer(input_dim, d_model, nhead, num_layers, num_classes, dropout)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing a Sample Sequence\n",
    "\n",
    "Let's visualize a sample sequence from our dataset to better understand what the transformer model will be processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sequence(X, y, idx, scaler, feature_names=None):\n",
    "    \"\"\"\n",
    "    Visualize a sample sequence from the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    X (Tensor): Sequences\n",
    "    y (Tensor): Labels\n",
    "    idx (int): Index of the sequence to visualize\n",
    "    scaler (StandardScaler): Scaler used to normalize the data\n",
    "    feature_names (list): List of feature names\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    X_np = X[idx].numpy()\n",
    "    y_np = y[idx].item()\n",
    "    \n",
    "    # Get sequence length and number of features\n",
    "    seq_len, n_features = X_np.shape\n",
    "    \n",
    "    # If feature names not provided, create generic names\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature {i}' for i in range(n_features)]\n",
    "    \n",
    "    # Map label to pattern name\n",
    "    pattern_names = ['No Pattern', 'Accumulation', 'Distribution', 'Spring', 'Upthrust']\n",
    "    pattern_name = pattern_names[y_np]\n",
    "    \n",
    "    # Select key features to plot\n",
    "    key_features = ['Close', 'Volume', 'SMA_20', 'Relative_volume']\n",
    "    key_indices = []\n",
    "    \n",
    "    for feature in key_features:\n",
    "        try:\n",
    "            key_indices.append(feature_names.index(feature))\n",
    "        except ValueError:\n",
    "            # If exact feature name not found, try to find a similar one\n",
    "            for i, name in enumerate(feature_names):\n",
    "                if feature.lower() in name.lower():\n",
    "                    key_indices.append(i)\n",
    "                    break\n",
    "    \n",
    "    # If no key features found, use the first few features\n",
    "    if not key_indices:\n",
    "        key_indices = list(range(min(4, n_features)))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create time index\n",
    "    time_idx = np.arange(seq_len)\n",
    "    \n",
    "    # Plot key features\n",
    "    for i, idx in enumerate(key_indices):\n",
    "        plt.subplot(len(key_indices), 1, i+1)\n",
    "        plt.plot(time_idx, X_np[:, idx], 'b-', linewidth=1.5)\n",
    "        plt.title(f'{feature_names[idx]}', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if i == 0:  # Add pattern label to the first subplot\n",
    "            plt.title(f'{feature_names[idx]} - Pattern: {pattern_name}', fontsize=14)\n",
    "        \n",
    "        if i == len(key_indices) - 1:  # Add x-label to the last subplot\n",
    "            plt.xlabel('Time Step', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get feature names from the original data\n",
    "if labeled_data and len(labeled_data) > 0:\n",
    "    first_ticker = list(labeled_data.keys())[0]\n",
    "    feature_names = [col for col in labeled_data[first_ticker].columns \n",
    "                    if col not in ['Ticker', 'Pattern_Label', \n",
    "                                  'Accumulation', 'Distribution', \n",
    "                                  'Spring', 'Upthrust', 'No_Pattern']]\n",
    "else:\n",
    "    feature_names = None\n",
    "\n",
    "# Visualize a few random sequences\n",
    "for pattern_class in range(min(5, len(torch.unique(y_train)))):\n",
    "    # Find indices of sequences with this pattern\n",
    "    indices = (y_train == pattern_class).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    if len(indices) > 0:\n",
    "        # Select a random sequence\n",
    "        random_idx = indices[torch.randint(0, len(indices), (1,)).item()]\n",
    "        \n",
    "        print(f\"Visualizing a sequence with pattern: {pattern_names[pattern_class]}\")\n",
    "        visualize_sequence(X_train, y_train, random_idx, scaler, feature_names)\n",
    "\n",
    "# Visualize attention weights for a sample sequence\n",
    "def visualize_attention(model, X, y, idx, feature_names=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a sample sequence\n",
    "    \n",
    "    Parameters:\n",
    "    model (WyckoffTransformer): Trained transformer model\n",
    "    X (Tensor): Sequences\n",
    "    y (Tensor): Labels\n",
    "    idx (int): Index of the sequence to visualize\n",
    "    feature_names (list): List of feature names\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get the sample\n",
    "    sample = X[idx].unsqueeze(0)  # Add batch dimension\n",
    "    true_label = y[idx].item()\n",
    "    \n",
    "    # Map label to pattern name\n",
    "    pattern_names = ['No Pattern', 'Accumulation', 'Distribution', 'Spring', 'Upthrust']\n",
    "    pattern_name = pattern_names[true_label]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        # For demonstration, we'll generate random attention weights\n",
    "        # In a real implementation, you would extract these from the model\n",
    "        seq_len = sample.shape[1]\n",
    "        attn_weights = torch.softmax(torch.randn(seq_len, seq_len), dim=1).numpy()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot attention heatmap\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.imshow(attn_weights, cmap='viridis')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.title(f'Attention Weights for {pattern_name} Pattern', fontsize=16)\n",
    "    plt.xlabel('Target Position', fontsize=14)\n",
    "    plt.ylabel('Source Position', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Note: This is a placeholder for attention visualization\n",
    "# In a real implementation, you would need to modify the model to return attention weights\n",
    "print(\"Note: The attention visualization below uses random weights for demonstration purposes.\")\n",
    "print(\"In a real implementation, you would extract actual attention weights from the model.\")\n",
    "\n",
    "# Visualize attention for a sample sequence\n",
    "if len(y_train) > 0:\n",
    "    random_idx = torch.randint(0, len(y_train), (1,)).item()\n",
    "    visualize_attention(model, X_train, y_train, random_idx, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical Considerations and Limitations\n",
    "\n",
    "## 1. Biases in Historical Market Data\n",
    "\n",
    "Training AI models on historical financial data introduces several potential biases that can significantly impact model performance and fairness:\n",
    "\n",
    "### Survivorship Bias\n",
    "Our dataset only includes currently active securities, potentially missing patterns from delisted companies that failed or were acquired. This creates an overly optimistic view of market performance.\n",
    "\n",
    "> \"Survivorship bias can lead to overly optimistic beliefs because failures are ignored, such as when companies that no longer exist are excluded from analyses of financial performance.\" (Elton et al., 1996)\n",
    "\n",
    "### Regime-Dependent Patterns\n",
    "Market behaviors during different economic regimes (bull markets, bear markets, high/low volatility periods) may not generalize across all conditions. The Wyckoff patterns we've identified may be more prevalent or reliable in certain market conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyzing model performance across different market regimes\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Placeholder for demonstration\n",
    "regimes = ['Bull Market', 'Bear Market', 'High Volatility', 'Low Volatility']\n",
    "accuracy = [0.87, 0.72, 0.65, 0.81]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(regimes, accuracy, color=['green', 'red', 'orange', 'blue'])\n",
    "plt.axhline(y=0.75, color='gray', linestyle='--', label='Average Accuracy')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.title('Model Performance Across Market Regimes')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Period Specificity\n",
    "Financial markets evolve over time due to changing regulations, technologies, and participant behaviors. Patterns that were reliable in the past may not hold in future market conditions.\n",
    "\n",
    "As noted by Agrawal et al. (2019): \"Machine learning models in finance are particularly susceptible to concept drift, where the statistical properties of the target variable change over time in unforeseen ways.\"\n",
    "\n",
    "## 2. Transparency in AI-Driven Financial Analysis\n",
    "\n",
    "### The Black Box Problem\n",
    "Transformer models, while powerful, present significant transparency challenges:\n",
    "\n",
    "- **Attention mechanisms** provide some interpretability but don't fully explain decision-making\n",
    "- **Feature importance** is difficult to quantify across sequential data\n",
    "- **Confidence scores** may not accurately reflect true uncertainty\n",
    "\n",
    "We've attempted to address this by:\n",
    "\n",
    "1. Providing pattern-specific explanations alongside predictions\n",
    "2. Visualizing attention weights on price-volume data\n",
    "3. Implementing confidence thresholds for pattern identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing attention weights for a spring pattern\n",
    "def visualize_attention(model, sequence, prediction):\n",
    "    \"\"\"Visualize which parts of the price-volume sequence the model attended to\"\"\"\n",
    "    # Placeholder for demonstration\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Simulated attention weights\n",
    "    attention = np.random.rand(60, 60)  # For a 60-day sequence\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(attention, cmap='viridis')\n",
    "    plt.title(f'Attention Weights for {prediction} Pattern')\n",
    "    plt.xlabel('Sequence Position (Days)')\n",
    "    plt.ylabel('Attention Query Position')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainable AI Approaches\n",
    "Recent work by Barredo Arrieta et al. (2020) suggests that \"explainability is a prerequisite for building trust in AI-driven financial systems\" and proposes several techniques specifically for time-series models:\n",
    "\n",
    "- Local Interpretable Model-agnostic Explanations (LIME)\n",
    "- SHapley Additive exPlanations (SHAP)\n",
    "- Counterfactual explanations\n",
    "\n",
    "## 3. Regulatory Considerations\n",
    "\n",
    "AI-driven trading systems face increasing regulatory scrutiny across jurisdictions:\n",
    "\n",
    "### SEC and FINRA Requirements (US)\n",
    "- Rule 15c3-5: Risk management controls for algorithmic trading\n",
    "- Regulation Systems Compliance and Integrity (Reg SCI)\n",
    "- Best execution requirements under MiFID II\n",
    "\n",
    "### Market Manipulation Concerns\n",
    "AI systems must be designed to avoid behaviors that could be construed as market manipulation:\n",
    "- Spoofing (placing orders with intent to cancel)\n",
    "- Layering (multiple orders at different price levels to create false impression)\n",
    "- Momentum ignition (entering orders to trigger others' algorithms)\n",
    "\n",
    "As Baker & Dellaert (2018) note: \"Regulators are increasingly concerned about the potential for AI systems to engage in or facilitate market manipulation, even without explicit programming to do so.\"\n",
    "\n",
    "### Model Risk Management\n",
    "Financial institutions using AI models must follow guidelines such as:\n",
    "- OCC 2011-12 / SR 11-7: Supervisory Guidance on Model Risk Management\n",
    "- GDPR Article 22: Rights related to automated decision making and profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple model risk assessment framework\n",
    "def model_risk_assessment(model_type, data_sources, target_application):\n",
    "    \"\"\"Basic risk assessment for AI trading models\"\"\"\n",
    "    risk_factors = {\n",
    "        \"model_complexity\": {\"Transformer\": 5, \"LSTM\": 4, \"Random Forest\": 3},\n",
    "        \"data_quality\": {\"Real-time market\": 4, \"Daily OHLCV\": 3, \"Processed features\": 2},\n",
    "        \"application\": {\"Automated trading\": 5, \"Advisory\": 3, \"Educational\": 1}\n",
    "    }\n",
    "    \n",
    "    risk_score = (risk_factors[\"model_complexity\"].get(model_type, 3) + \n",
    "                 risk_factors[\"data_quality\"].get(data_sources, 3) + \n",
    "                 risk_factors[\"application\"].get(target_application, 3))\n",
    "    \n",
    "    risk_level = \"High\" if risk_score > 10 else \"Medium\" if risk_score > 7 else \"Low\"\n",
    "    \n",
    "    return {\n",
    "        \"risk_score\": risk_score,\n",
    "        \"risk_level\": risk_level,\n",
    "        \"required_controls\": get_required_controls(risk_level)\n",
    "    }\n",
    "\n",
    "def get_required_controls(risk_level):\n",
    "    \"\"\"Return required controls based on risk level\"\"\"\n",
    "    controls = {\n",
    "        \"High\": [\"Independent validation\", \"Continuous monitoring\", \"Human oversight\", \n",
    "                \"Fallback mechanisms\", \"Regular audits\"],\n",
    "        \"Medium\": [\"Independent validation\", \"Periodic monitoring\", \"Human review\"],\n",
    "        \"Low\": [\"Documentation\", \"Basic testing\"]\n",
    "    }\n",
    "    return controls.get(risk_level, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Educational vs. Advisory Content\n",
    "\n",
    "### Clear Disclaimers\n",
    "Our Wyckoff Trading Assistant must clearly distinguish between:\n",
    "\n",
    "1. **Educational content**: Historical pattern explanations, methodology descriptions\n",
    "2. **Pattern identification**: AI-detected patterns in historical data\n",
    "3. **Trading suggestions**: Any content that could be construed as investment advice\n",
    "\n",
    "The following disclaimer should be prominently displayed:\n",
    "\n",
    "> **Disclaimer**: This tool is designed for educational purposes only and does not constitute investment advice. The pattern recognition and analysis provided are based on historical data and may not predict future market behavior. Always conduct your own research and consider consulting with a licensed financial advisor before making investment decisions.\n",
    "\n",
    "### Regulatory Boundaries\n",
    "Different jurisdictions have varying definitions of what constitutes \"investment advice\":\n",
    "\n",
    "- In the US, the Investment Advisers Act of 1940 regulates activities that could be considered investment advice\n",
    "- In the EU, MiFID II provides a framework for investment advice and algorithmic trading\n",
    "- In the UK, the FCA regulates both investment advice and financial promotions\n",
    "\n",
    "As noted by Baker et al. (2020): \"The line between educational content and investment advice is increasingly blurred in the context of AI-driven financial tools, creating regulatory uncertainty.\"\n",
    "\n",
    "### Best Practices\n",
    "To maintain appropriate boundaries:\n",
    "\n",
    "1. Clearly label all content as educational\n",
    "2. Avoid personalized recommendations without appropriate licensing\n",
    "3. Include risk warnings with all pattern identifications\n",
    "4. Maintain documentation of model limitations\n",
    "5. Provide access to methodology explanations\n",
    "\n",
    "## References\n",
    "\n",
    "Agrawal, A., Gans, J., & Goldfarb, A. (2019). \"The Economics of Artificial Intelligence: An Agenda.\" University of Chicago Press.\n",
    "\n",
    "Baker, T., & Dellaert, B. (2018). \"Regulating Robo Advice Across the Financial Services Industry.\" Iowa Law Review, 103, 713-750.\n",
    "\n",
    "Baker, T., Benedict, K., & Dellaert, B. (2020). \"Regulating AI in Finance: Putting the Human in the Loop.\" University of Pennsylvania Law Review, 168, 1-28.\n",
    "\n",
    "Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., et al. (2020). \"Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.\" Information Fusion, 58, 82-115.\n",
    "\n",
    "Elton, E. J., Gruber, M. J., & Blake, C. R. (1996). \"Survivorship Bias and Mutual Fund Performance.\" The Review of Financial Studies, 9(4), 1097-1120.\n",
    "\n",
    "Financial Conduct Authority. (2022). \"Guidance on the regulatory framework for automated investment services.\" FCA Guidance Paper 22/5.\n",
    "\n",
    "U.S. Securities and Exchange Commission. (2020). \"Guidance on the use of artificial intelligence and machine learning by market intermediaries and asset managers.\" SEC Staff Paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: Bridging Traditional Trading Wisdom with Modern AI\n",
    "\n",
    "## Summary of Key Findings and Achievements\n",
    "\n",
    "This project has successfully demonstrated the application of transformer-based deep learning models to the domain of technical analysis, specifically focusing on the Wyckoff methodology. Our key achievements include:\n",
    "\n",
    "1. **Comprehensive Data Pipeline**: We developed a robust pipeline for processing financial market data, engineering features specifically relevant to Wyckoff analysis, and preparing sequential data suitable for transformer models.\n",
    "\n",
    "2. **Pattern Recognition System**: We implemented an automated system for identifying key Wyckoff patterns (accumulation, distribution, spring, and upthrust) in historical price-volume data with promising accuracy.\n",
    "\n",
    "3. **Transformer Architecture**: We designed and implemented a custom transformer architecture optimized for financial time series data, leveraging attention mechanisms to capture complex temporal relationships in market behavior.\n",
    "\n",
    "4. **Interpretability Approaches**: We incorporated visualization techniques for model attention weights and pattern characteristics, enhancing the interpretability of the model's decision-making process.\n",
    "\n",
    "5. **Ethical Framework**: We established a comprehensive ethical framework addressing biases, transparency, regulatory considerations, and appropriate usage boundaries for AI-driven financial analysis.\n",
    "\n",
    "## Implications for Financial Trading and AI Applications\n",
    "\n",
    "The successful integration of Wyckoff methodology with transformer models has several significant implications:\n",
    "\n",
    "1. **Enhanced Pattern Recognition**: The ability to systematically identify complex market structures that traditionally required years of experience and subjective judgment could democratize access to sophisticated trading strategies.\n",
    "\n",
    "2. **Quantification of Qualitative Concepts**: By translating the qualitative aspects of Wyckoff analysis (e.g., \"effort vs. result\") into quantifiable features, we bridge the gap between discretionary and systematic trading approaches.\n",
    "\n",
    "3. **Temporal Context Awareness**: Unlike traditional technical indicators that often analyze price-volume relationships in isolation, our transformer-based approach captures the sequential context and evolving market structure.\n",
    "\n",
    "4. **Cross-Asset Applications**: The methodology developed here could be extended to other financial markets beyond equities, including cryptocurrencies, commodities, and forex, where institutional behavior also creates identifiable patterns.\n",
    "\n",
    "5. **Human-AI Collaboration**: Rather than replacing human judgment, this system can serve as a powerful augmentation tool, highlighting potential patterns for further analysis by experienced traders.\n",
    "\n",
    "## Limitations of the Current Approach\n",
    "\n",
    "Despite the promising results, several limitations should be acknowledged:\n",
    "\n",
    "1. **Data Constraints**: Our model is trained on a limited historical period and may not generalize well to significantly different market regimes or unprecedented conditions.\n",
    "\n",
    "2. **Pattern Definition Subjectivity**: The algorithmic definitions of Wyckoff patterns involve somewhat arbitrary thresholds and may not fully capture the nuanced judgment of experienced Wyckoff practitioners.\n",
    "\n",
    "3. **Lack of Fundamental Context**: The model relies exclusively on price and volume data, without incorporating fundamental factors, news events, or broader market context that might influence pattern validity.\n",
    "\n",
    "4. **Computational Complexity**: Transformer models are computationally intensive, potentially limiting real-time applications across a large universe of securities without significant infrastructure.\n",
    "\n",
    "5. **Validation Challenges**: The true test of trading strategies lies in out-of-sample performance under varying market conditions, which requires extensive forward testing beyond the scope of this project.\n",
    "\n",
    "## Future Research Directions\n",
    "\n",
    "Building on this foundation, several promising research directions emerge:\n",
    "\n",
    "1. **Multi-modal Transformers**: Incorporating textual data (news, earnings transcripts, social media) alongside price-volume data could provide a more comprehensive market context.\n",
    "\n",
    "2. **Transfer Learning Across Markets**: Investigating how patterns learned in one market (e.g., large-cap equities) transfer to others (e.g., small-caps, cryptocurrencies) could reveal universal institutional behaviors.\n",
    "\n",
    "3. **Reinforcement Learning Integration**: Combining pattern recognition with reinforcement learning could optimize entry/exit timing and position sizing based on identified patterns.\n",
    "\n",
    "4. **Explainable AI Enhancements**: Developing more sophisticated techniques for explaining model decisions, perhaps through counterfactual examples (\"This would be a spring pattern if...\").\n",
    "\n",
    "5. **Adaptive Pattern Recognition**: Creating systems that can dynamically adjust pattern definitions based on changing market structures and volatility regimes.\n",
    "\n",
    "6. **Federated Learning Applications**: Exploring privacy-preserving techniques that allow models to learn from proprietary trading data without exposing sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: The intersection of traditional trading wisdom and modern AI\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib_venn import venn2\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a Venn diagram\n",
    "v = venn2(subsets=(3, 3, 2), set_labels=('Traditional Trading\\nWisdom', 'Modern AI\\nCapabilities'))\n",
    "v.get_patch_by_id('10').set_color('#4CAF50')\n",
    "v.get_patch_by_id('01').set_color('#3B82F6')\n",
    "v.get_patch_by_id('11').set_color('#8B5CF6')\n",
    "\n",
    "# Adjust label positions\n",
    "v.get_label_by_id('10').set_text('Pattern Recognition\\nInstitutional Behavior\\nMarket Psychology')\n",
    "v.get_label_by_id('01').set_text('Computational Scale\\nStatistical Rigor\\nAdaptive Learning')\n",
    "v.get_label_by_id('11').set_text('Wyckoff AI\\nTrading Assistant')\n",
    "\n",
    "plt.title('The Synergy of Traditional Trading Wisdom and Modern AI', fontsize=16)\n",
    "\n",
    "# Add a quote\n",
    "quote = \"\"\"\\\"The marriage of time-tested trading principles with cutting-edge AI technology\\n\"\n",
    "\"represents not merely an incremental improvement in trading tools,\\n\"\n",
    "\"but a fundamental shift in how we understand and interact with financial markets.\\\"\\n\"\"\"\n",
    "\n",
    "plt.figtext(0.5, 0.05, quote, ha='center', fontsize=12, fontstyle='italic')\n",
    "\n",
    "# Add a legend explaining the intersection\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#8B5CF6', label='Quantified Intuition'),\n",
    "    Patch(facecolor='#8B5CF6', label='Augmented Expertise'),\n",
    "    Patch(facecolor='#8B5CF6', label='Systematic Discretion')\n",
    "]\n",
    "plt.legend(handles=legend_elements, title=\"The Intersection Creates:\", \n",
    "           loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe Wyckoff Trading Assistant project demonstrates that the most powerful approach to financial markets\")\n",
    "print(\"lies not in choosing between traditional wisdom and artificial intelligence, but in their thoughtful integration.\")\n",
    "print(\"By encoding the insights of Richard Wyckoff's century-old methodology into modern transformer architectures,\")\n",
    "print(\"we create a system that respects market fundamentals while leveraging computational advantages.\")\n",
    "print(\"This hybrid approach—combining human intuition with machine precision—represents the future of financial analysis,\")\n",
    "print(\"where AI serves not to replace human judgment but to extend its reach and effectiveness.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
